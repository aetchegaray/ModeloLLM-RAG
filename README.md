ğŸ“˜ Asistente de PolÃ­ticas Internas â€” Proyecto RAG con LLM + FastAPI + Streamlit
ğŸ§  DescripciÃ³n del Proyecto

Este proyecto consiste en desarrollar una aplicaciÃ³n tipo chatbot que permita consultar un reglamento de polÃ­ticas internas utilizando lenguaje natural. La soluciÃ³n implementa una arquitectura RAG (Retrieval-Augmented Generation) basada en:

- RecuperaciÃ³n de contexto mediante FAISS y embeddings (e5-large-v2)
- GeneraciÃ³n de respuestas con un modelo LLM (Mistral-7B-Instruct v0.2)
- Backend desarrollado en FastAPI
- Interfaz de usuario con Streamlit
- DockerizaciÃ³n completa para facilitar su ejecuciÃ³n y portabilidad.

ğŸ¯ Objetivos:
- Implementar un backend en FastAPI que reciba preguntas y genere respuestas en base a un reglamento interno.
- Construir una interfaz simple en Streamlit para consultas por parte del usuario.
- Procesar un documento PDF en texto estructurado para construir un Ã­ndice vectorial con embeddings.
- Permitir una ejecuciÃ³n rÃ¡pida y portable usando Docker.
- Permitir ejecuciÃ³n 100% offline al incluir modelos predescargados.


âš™ï¸ Requisitos:
Tener Docker instalado y funcionando.
Tener Python 3.9+ instalado solo si se desea ejecutar build_index.py fuera del contenedor.
Tener cuenta en Hugging Face y ejecutar huggingface-cli login si se usa modo online.
ConexiÃ³n a internet para descargar modelos desde Hugging Face la primera vez.

ğŸš€ Instrucciones de EjecuciÃ³n (modo Docker):

1. Clonar el repositorio en la instancia.
2. Construir el Ã­ndice vectorial FAISS (una vez): python build_index.py Esto transforma el PDF en chunks, genera embeddings y guarda el Ã­ndice FAISS.
3. Construir la imagen Docker: bash build_api.sh
(Internamente corre: docker build -t asistente-politicas .)
4. Ejecutar la aplicaciÃ³n en contenedor: bash run_docker.sh
(Internamente corre: docker run -p 8501:8501 -p 8000:8000 asistente-politicas)

Acceder desde el navegador:
Interfaz de usuario (Streamlit): http://localhost:8501
API (FastAPI): http://localhost:8000/docs

ğŸŒ Modo Online: La aplicaciÃ³n puede funcionar descargando los modelos directamente desde Hugging Face si tiene conexiÃ³n a internet y autenticaciÃ³n previa con huggingface-cli login.

LLM: mistralai/Mistral-7B-Instruct-v0.1
Embeddings: intfloat/e5-large-v2

ğŸ“´ Modo Offline:
Si se desea ejecutar sin conexiÃ³n, se pueden descargar previamente los modelos y colocarlos en:

models/
â”œâ”€â”€ mistral-7b-instruct/
â”œâ”€â”€ e5-large-v2/

El cÃ³digo detecta automÃ¡ticamente si existen localmente y los carga en modo offline (local_files_only=True). Si el repositorio clonado no incluye los modelos, se puede ejecutar download_models.py previamente con conexiÃ³n, o montar la carpeta models/ en la raÃ­z del proyecto.

ğŸ“Œ Herramientas Utilizadas:
FastAPI
Streamlit
Hugging Face Transformers
LangChain
Sentence Transformers
FAISS
Docker